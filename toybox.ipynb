{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sim import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e960a3",
   "metadata": {},
   "source": [
    "## Toy Oscillation Model\n",
    "\n",
    "This is the toy example from the <a href=\"https://arxiv.org/physics/9711021\">Feldman and Cousins paper</a>. We make an oscillation parameter measurement for electron neutrino appearance. The model we'd like to fit is:\n",
    "\n",
    "$$\n",
    "P(\\nu_\\mu \\rightarrow \\nu_e) = \\sin^2(2\\theta) \\sin^2(1.27 \\Delta m^2 L / E)\n",
    "$$\n",
    "\n",
    "In this toy experiment, neutrinos are produced via pion decays uniformly in a region 600–1000m from the detector. We expect a 100 event background in each of 5 energy bins spanning the region 10–60 GeV. We assume that the flux of initial $\\nu_\\mu$ is such that if $P(\\nu_\\mu \\rightarrow \\nu_e) = 0.01$ averaged over any bin, then that bin has an expected additional contribution of 100 events due to electron appearance.\n",
    "\n",
    "The simulation has been conveniently implemented in `sim.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin2_2theta = 0.9\n",
    "delta_m2 = 10. # eV^2\n",
    "\n",
    "counts, expected_signal = simulate_counts(\n",
    "    sin2_2theta=sin2_2theta,\n",
    "    delta_m2=delta_m2\n",
    ")\n",
    "plot_event_display(\n",
    "    sin2_2theta=sin2_2theta,\n",
    "    delta_m2=delta_m2,\n",
    "    counts=counts,\n",
    "    expected_signal=expected_signal,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a682fd",
   "metadata": {},
   "source": [
    "## Traditional Feldman-Cousins\n",
    "\n",
    "To determine the acceptance region of a point in the $\\sin^2(2\\theta) - \\Delta m^2$ plane, we:\n",
    "\n",
    "1. Use MC to simulate the results of a large number of experiments\n",
    "\n",
    "2. Compute \n",
    "   $$\n",
    "   \\Delta \\chi^2 = 2 \\sum_i \\left[ \\mu_i - \\mu_{\\text{best}_i} + n_i \\ln \\left( \\frac{\\mu_{\\text{best}_i} + b_i}{\\mu_i + b_i} \\right) \\right],\n",
    "   $$\n",
    "   with $\\mu_i$ the calculated expected oscillation contribution to the number of events, $b_i$ the known mean expected background, and $n_i$ the measured data in each of the bins. *Note: in this case, $b_i = 100$, $\\mu_{\\text{best}_i} = n_i - 100$. In more complicated experimental fits, $\\mu_{\\text{best}_i}$ can be computed via MLE or something similar.*\n",
    "\n",
    "3. Within each grid point, compute a critical $\\Delta \\chi^2_c$ such that a fraction $\\alpha$ of the $K$ simulated experiments have $\\Delta \\chi^2 < \\Delta \\chi^2_c$.\n",
    "\n",
    "4. Compute $\\Delta \\chi^2$ for the observed experimental data $N = \\{n_i\\}$, and then admit a point into the acceptance region if\n",
    "\n",
    "   $$\n",
    "   \\Delta \\chi^2(N \\mid \\sin^2(2\\theta), \\Delta m^2) < \\Delta \\chi^2_c(\\sin^2(2\\theta), \\Delta m^2).\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi2 infrastructure\n",
    "def compute_delta_chi2(x, sin2_2theta, delta_m2):\n",
    "    out = 0.\n",
    "    for i, E in enumerate(ENERGY_CENTERS):\n",
    "        # getting mu_i, expected oscillation contributions\n",
    "        P_avg = average_probability_over_L(sin2_2theta, delta_m2, E)\n",
    "        mu_i = NORMALIZATION * P_avg\n",
    "        \n",
    "        # getting mu_best_i\n",
    "        mu_best_i = x[i] - BACKGROUND_PER_BIN\n",
    "        \n",
    "        # update sum\n",
    "        out += 2 * (mu_i - mu_best_i + x[i]*np.log((mu_best_i+BACKGROUND_PER_BIN)/(mu_i+BACKGROUND_PER_BIN)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccbd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example dchi2 distribution\n",
    "sin2_2theta = 0.01\n",
    "delta_m2 = 800. # eV^2\n",
    "x, _ = simulate_counts(\n",
    "    sin2_2theta=sin2_2theta,\n",
    "    delta_m2=delta_m2\n",
    ")\n",
    "\n",
    "GRIDSIZE = 30\n",
    "LOG_SIN2_2THETA_BOUNDS = (-4., 0.)\n",
    "LOG_DELTA_M2_BOUNDS = (0., 3.)\n",
    "log_sin2_2theta_grid = np.linspace(*LOG_SIN2_2THETA_BOUNDS, GRIDSIZE)\n",
    "log_delta_m2_grid = np.linspace(*LOG_DELTA_M2_BOUNDS, GRIDSIZE)\n",
    "THETAS = np.array([[logsinsq, logdmsq] for logsinsq in log_sin2_2theta_grid for logdmsq in log_delta_m2_grid])\n",
    "\n",
    "delta_chi2s = np.array([compute_delta_chi2(x, *(10**th)) for th in tqdm(THETAS)])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,6), dpi=100)\n",
    "contour = ax.tricontourf(THETAS[:, 0], THETAS[:, 1], np.log10(delta_chi2s), levels=20, cmap='viridis_r')\n",
    "cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r\"$\\log_{10} \\Delta \\chi^2 $\")  # Set colorbar label\n",
    "ax.scatter(np.log10(sin2_2theta), np.log10(delta_m2), color='white', label='Truth', marker='*', s=80)\n",
    "ax.set_xlabel(r'$\\log_{10} \\sin^2 2 \\theta$')\n",
    "ax.set_ylabel(r'$\\log_{10} \\Delta m^2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing critical dchi2 values. you only have to do this once!\n",
    "ALPHA = 0.9\n",
    "K = 1000\n",
    "delta_chi2_crit_dict = {\n",
    "    \"sin2_2theta\": [],\n",
    "    \"delta_m2\": [],\n",
    "    \"delta_chi2_crit\": [],\n",
    "}\n",
    "\n",
    "for log10_sin2_2theta, log10_delta_m2 in tqdm(THETAS):\n",
    "    # simulate K experiments and compute dchi2s\n",
    "    dchi2s_temp = []\n",
    "    for _ in range(K):\n",
    "        x_temp, _ = simulate_counts(\n",
    "            sin2_2theta=10.**log10_sin2_2theta,\n",
    "            delta_m2=10.**log10_delta_m2\n",
    "        )\n",
    "        dchi2_temp = compute_delta_chi2(\n",
    "            x=x_temp,\n",
    "            sin2_2theta=10.**log10_sin2_2theta,\n",
    "            delta_m2=10.**log10_delta_m2,\n",
    "        )\n",
    "        dchi2s_temp.append(dchi2_temp)\n",
    "        \n",
    "    # compute alpha percentile and append\n",
    "    delta_chi2_crit = np.quantile(dchi2s_temp, ALPHA)\n",
    "    delta_chi2_crit_dict[\"sin2_2theta\"].append(10.**log10_sin2_2theta)\n",
    "    delta_chi2_crit_dict[\"delta_m2\"].append(10.**log10_delta_m2)\n",
    "    delta_chi2_crit_dict[\"delta_chi2_crit\"].append(delta_chi2_crit)\n",
    "    \n",
    "delta_chi2_crit_df = pd.DataFrame(delta_chi2_crit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b55d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feldman-cousins infrastructure\n",
    "delta_chi2_df = pd.DataFrame({\n",
    "    \"sin2_2theta\": 10.**THETAS[:, 0],\n",
    "    \"delta_m2\": 10.**THETAS[:, 1],\n",
    "    \"delta_chi2\": delta_chi2s,\n",
    "})\n",
    "\n",
    "acceptance_fc_df = pd.merge(delta_chi2_df, delta_chi2_crit_df, on=['sin2_2theta', 'delta_m2'], how='inner')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6), dpi=100)\n",
    "ax.tricontour(\n",
    "    acceptance_fc_df[\"sin2_2theta\"],\n",
    "    acceptance_fc_df[\"delta_m2\"],\n",
    "    acceptance_fc_df[\"delta_chi2\"] - acceptance_fc_df[\"delta_chi2_crit\"],\n",
    "    levels=[0.],\n",
    "    colors='r'\n",
    ")\n",
    "\n",
    "contour = ax.tricontourf(\n",
    "    acceptance_fc_df[\"sin2_2theta\"],\n",
    "    acceptance_fc_df[\"delta_m2\"],\n",
    "    np.log10(acceptance_fc_df[\"delta_chi2\"]),\n",
    "    cmap='Greys',\n",
    "    levels=20\n",
    ")\n",
    "cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r\"$\\log_{10} \\Delta \\chi^2 $\")  # Set colorbar label\n",
    "\n",
    "sc1 = ax.scatter(sin2_2theta, delta_m2, marker='*', label='Truth')\n",
    "\n",
    "bfp = tuple(acceptance_fc_df[acceptance_fc_df[\"delta_chi2\"]==acceptance_fc_df[\"delta_chi2\"].min()][[\"sin2_2theta\", \"delta_m2\"]].values.flatten())\n",
    "\n",
    "sc2 = ax.scatter(*bfp, marker='o', facecolors='none', edgecolors='skyblue')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r\"$\\sin^2 2 \\theta$\")\n",
    "ax.set_ylabel(r\"$\\Delta m^2$ [eV]$^2$\")\n",
    "\n",
    "# Proxy for tricontour line\n",
    "from matplotlib.lines import Line2D\n",
    "contour_proxy = Line2D([0], [0], color='red', linestyle='-')\n",
    "\n",
    "# Combine legend handles and labels\n",
    "handles = [sc1, sc2, contour_proxy]\n",
    "labels = [\"Truth\", \"Best fit point\", f\"{100*ALPHA}% CL, Feldman-Cousins\"]\n",
    "\n",
    "ax.legend(handles, labels, framealpha=1., loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac282c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example, an exclusion\n",
    "sin2_2theta_2 = 0.0005\n",
    "delta_m2_2 = 1.5 # eV^2\n",
    "x_2, _ = simulate_counts(\n",
    "    sin2_2theta=sin2_2theta_2,\n",
    "    delta_m2=delta_m2_2\n",
    ")\n",
    "\n",
    "delta_chi2s = np.array([compute_delta_chi2(x_2, *(10**th)) for th in tqdm(THETAS)])\n",
    "\n",
    "delta_chi2_df = pd.DataFrame({\n",
    "    \"sin2_2theta\": 10.**THETAS[:, 0],\n",
    "    \"delta_m2\": 10.**THETAS[:, 1],\n",
    "    \"delta_chi2\": delta_chi2s,\n",
    "})\n",
    "\n",
    "acceptance_fc_2_df = pd.merge(delta_chi2_df, delta_chi2_crit_df, on=['sin2_2theta', 'delta_m2'], how='inner')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6), dpi=100)\n",
    "ax.tricontour(\n",
    "    acceptance_fc_2_df[\"sin2_2theta\"],\n",
    "    acceptance_fc_2_df[\"delta_m2\"],\n",
    "    acceptance_fc_2_df[\"delta_chi2\"] - acceptance_fc_2_df[\"delta_chi2_crit\"],\n",
    "    levels=[0.],\n",
    "    colors='r'\n",
    ")\n",
    "\n",
    "contour = ax.tricontourf(\n",
    "    acceptance_fc_2_df[\"sin2_2theta\"],\n",
    "    acceptance_fc_2_df[\"delta_m2\"],\n",
    "    np.log10(acceptance_fc_2_df[\"delta_chi2\"]),\n",
    "    cmap='Greys',\n",
    "    levels=20\n",
    ")\n",
    "cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r\"$\\log_{10} \\Delta \\chi^2 $\")  # Set colorbar label\n",
    "\n",
    "sc1 = ax.scatter(sin2_2theta_2, delta_m2_2, marker='*', label='Truth')\n",
    "\n",
    "bfp = tuple(acceptance_fc_2_df[acceptance_fc_2_df[\"delta_chi2\"]==acceptance_fc_2_df[\"delta_chi2\"].min()][[\"sin2_2theta\", \"delta_m2\"]].values.flatten())\n",
    "\n",
    "sc2 = ax.scatter(*bfp, marker='o', facecolors='none', edgecolors='skyblue')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r\"$\\sin^2 2 \\theta$\")\n",
    "ax.set_ylabel(r\"$\\Delta m^2$ [eV]$^2$\")\n",
    "\n",
    "# Proxy for tricontour line\n",
    "from matplotlib.lines import Line2D\n",
    "contour_proxy = Line2D([0], [0], color='red', linestyle='-')\n",
    "\n",
    "# Combine legend handles and labels\n",
    "handles = [sc1, sc2, contour_proxy]\n",
    "labels = [\"Truth\", \"Best fit point\", f\"{100*ALPHA}% CL, Feldman-Cousins\"]\n",
    "\n",
    "ax.legend(handles, labels, framealpha=1.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320921ae",
   "metadata": {},
   "source": [
    "## Implementation of Direct Amortized Neural Ratio Estimation\n",
    "\n",
    "From <a href=\"https://arxiv.org/abs/2311.10571\">2311.10571</a>.\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "Let's define some priors. Namely, $\\log \\sin^2 2 \\theta \\sim \\text{Unif} (-4, 0)$, $\\log \\Delta m^2 \\, [\\text{eV}^2] \\sim \\text{Unif} (0,3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_uniform_prior(n_samples):\n",
    "    log_sin2_2theta_samples = np.random.uniform(*LOG_SIN2_2THETA_BOUNDS, n_samples)\n",
    "    log_delta_m2_samples = np.random.uniform(*LOG_DELTA_M2_BOUNDS, n_samples)\n",
    "    return np.column_stack((log_sin2_2theta_samples, log_delta_m2_samples))\n",
    "\n",
    "samples_to_plot = generate_from_uniform_prior(n_samples=10000)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,6), dpi=100)\n",
    "h = ax.hist2d(samples_to_plot[:, 0], samples_to_plot[:, 1], vmin=0., cmap='Blues')\n",
    "ax.set_xlim(*LOG_SIN2_2THETA_BOUNDS)\n",
    "ax.set_ylim(*LOG_DELTA_M2_BOUNDS)\n",
    "ax.set_xlabel(r'$\\log \\sin^2 2 \\theta$')\n",
    "ax.set_ylabel(r'$\\log \\Delta m^2 ')\n",
    "\n",
    "plt.colorbar(h[3], ax=ax, label='Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling on simulator\n",
    "n_samples = 50000\n",
    "thetas = generate_from_uniform_prior(n_samples)\n",
    "theta_ps = generate_from_uniform_prior(n_samples)\n",
    "\n",
    "xs = []\n",
    "for theta in tqdm(thetas):\n",
    "    xs.append(simulate_counts(*(10**theta))[0]) # recall theta is the log of the osc params\n",
    "xs = np.array(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49837f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building labeled data\n",
    "data1 = np.column_stack((xs, thetas, theta_ps, np.ones(n_samples)))\n",
    "data0 = np.column_stack((xs, theta_ps, thetas, np.zeros(n_samples)))\n",
    "\n",
    "data = np.row_stack((data1, data0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building training and test-set data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = data[:, :-1]; y = data[:, -1]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d71c6",
   "metadata": {},
   "source": [
    "### Network training\n",
    "\n",
    "Build any classifier that you want. I just want something a bit straightforward and easy to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "link = inputs\n",
    "\n",
    "for _ in range(5):\n",
    "    link = layers.Dense(64, activation='relu')(link)\n",
    "\n",
    "pre_sigmoid = layers.Dense(1)(link)\n",
    "outputs = layers.Activation('sigmoid')(pre_sigmoid)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77972b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model that ends at the pre-sigmoid layer (useful later)\n",
    "pre_sigmoid_model = Model(inputs=model.input, outputs=pre_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5), dpi=100)\n",
    "\n",
    "axs[0].plot(history.history['loss'], label='Training Loss')\n",
    "axs[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Binary Cross-Entropy Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axs[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f57b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_test_preds = model.predict(X_test).flatten() > 0.5\n",
    "cm = confusion_matrix(y_test, y_test_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd68ae",
   "metadata": {},
   "source": [
    "### Uses of the Network: Likelihood Ratio and Posterior Density Estimation\n",
    "\n",
    "**Likelihood ratio estimation.** From <a href=\"https://arxiv.org/abs/2311.10571\">the paper</a>, if $d$ is our neural network, we have that:\n",
    "\n",
    "$$ \\frac{P (x | \\theta)}{P (x | \\theta')} = \\frac{d (x, \\theta, \\theta')}{1 - d (x, \\theta, \\theta') } $$\n",
    "\n",
    "The final activation layer is a sigmoid function:\n",
    "\n",
    "$$ \\sigma (x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Let $d_{\\phi - \\sigma}$ be the subnetwork consisting of all layers except the final activation. That is, $d (x, \\theta, \\theta') = \\sigma (d_{\\phi - \\sigma} (x, \\theta, \\theta'))$. Then:\n",
    "\n",
    "$$ \\frac{P (x | \\theta)}{P (x | \\theta')} = \\frac{d (x, \\theta, \\theta')}{1 - d (x, \\theta, \\theta') } = \\frac{\\sigma (d_{\\phi - \\sigma} (x, \\theta, \\theta') )}{1 - \\sigma (d_{\\phi - \\sigma} (x, \\theta, \\theta') )} = \\frac{\\frac{1}{1 + \\exp (- d_{\\phi - \\sigma} (x, \\theta, \\theta')) }}{1 - \\frac{1}{1 + \\exp (- d_{\\phi - \\sigma} (x, \\theta, \\theta')) }} = \\frac{1}{1 + \\exp (- d_{\\phi - \\sigma} (x, \\theta, \\theta')) - 1} = \\exp (  d_{\\phi - \\sigma} (x, \\theta, \\theta'))$$\n",
    "\n",
    "And of course, conveniently:\n",
    "\n",
    "$$ \\Delta LLH = \\log P (x | \\theta) - \\log P (x | \\theta') = d_{\\phi - \\sigma} (x, \\theta, \\theta'). $$\n",
    "\n",
    "**Posterior density estimation.** We cite Eq. 7:\n",
    "\n",
    "$$ \\log⁡ p(\\theta | x) = \\log p (\\theta) + \\log M - \\log \\sum_{i=1}^M \\exp [ - \\log r(x, \\theta, \\theta_i') ]$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$ \\log p(\\theta | x) = \\log p (\\theta) + \\log M - \\log \\sum_{i=1}^M \\exp [ - \\log \\exp d_{\\phi - \\sigma} (x , \\theta, \\theta') ] = \\log p (\\theta) + \\log M - \\log \\sum_{i=1}^M \\exp [ - d_{\\phi - \\sigma} (x, \\theta, \\theta') ]. $$\n",
    "\n",
    "For numerical stability, consider $a_i$ and $\\overline{a} = \\max_i a_i$ such that:\n",
    "\n",
    "$$ \\log \\sum_i \\exp a_i = \\log \\sum_i \\exp (\\overline{a} - \\overline{a} + a_i)  =  \\log \\sum_i \\exp ( \\overline{a}) \\exp (- \\overline{a} + a_i) = \\log \\bigg{[} \\exp (\\overline{a}) \\sum_i \\exp (-\\overline{a} + a_i ) \\bigg{]} = \\overline{a} + \\log \\sum_i \\exp (- \\overline{a} + a_i)$$\n",
    "\n",
    "To build the approximate test statistic for a Feldman-Cousins stepthrough, I will use the maximum _a posteriori_ as the best fit point estimate for an experimental realization and use this grid point for the denominator of the likelihood ratio estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fill_neg_inf_with_min(arr):\n",
    "    \"\"\"Fills negative infinities in a NumPy array with its smallest value.\n",
    "\n",
    "    Args:\n",
    "        arr (np.ndarray): The input NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A new array with negative infinities replaced.\n",
    "    \"\"\"\n",
    "    arr = np.array(arr, dtype=float)\n",
    "    b = np.sort(arr)\n",
    "    min_val = b[~np.isneginf(b)][1]\n",
    "    arr[np.isneginf(arr)] = min_val\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _make_network_input(inpx, inpth, inpthp, scaler):\n",
    "    inp = np.hstack((inpx, inpth, inpthp))\n",
    "    inp_scaled = scaler.transform(inp)\n",
    "    return inp_scaled   \n",
    "\n",
    "\n",
    "def compute_log_posterior(thetas, x, M, pre_sigmoid_model, scaler):\n",
    "    \"\"\"\n",
    "    thetas : np.ndarray\n",
    "        List of grid points in model parameter space on which to evaluate posterior density\n",
    "    x : array-like\n",
    "        Raw experimental realization\n",
    "    M : int\n",
    "        Number of samples to use in MC mean\n",
    "    pre_sigmoid_model\n",
    "        Trained subnetwork consisting of all layers except final activation\n",
    "    scaler\n",
    "        MinMaxScaler used for input data preparation\n",
    "    \"\"\"\n",
    "    N = thetas.shape[0]\n",
    "    \n",
    "    # using flat prior\n",
    "    log_prior = np.log(1 / (\n",
    "        (LOG_SIN2_2THETA_BOUNDS[1] - LOG_SIN2_2THETA_BOUNDS[0]) * \\\n",
    "        (LOG_DELTA_M2_BOUNDS[1] - LOG_DELTA_M2_BOUNDS[0])\n",
    "    ))\n",
    "    log_M = np.log(M)\n",
    "    \n",
    "    # sample prior M times, shared across all thetas\n",
    "    theta_ps = generate_from_uniform_prior(M)  # shape: (M, prior_dim)\n",
    "\n",
    "    # prepare nn inputs\n",
    "    inpx = np.repeat(np.array([x]), N * M, axis=0)  # shape: (N*M, x_dim)\n",
    "    inpth = np.repeat(thetas, M, axis=0)            # shape: (N*M, 2)\n",
    "    theta_ps_tiled = np.tile(theta_ps, (N, 1))      # shape: (N*M, prior_dim)\n",
    "\n",
    "    inp_scaled = _make_network_input(inpx, inpth, theta_ps_tiled, scaler) # shape: (N*M, total_input_dim)\n",
    "\n",
    "    # predict\n",
    "    pre_sigmoid_preds = pre_sigmoid_model.predict_on_batch(inp_scaled)  # shape: (N*M, 1) or (N*M,)\n",
    "    pre_sigmoid_preds = pre_sigmoid_preds.reshape(N, M)\n",
    "\n",
    "    # logpost\n",
    "    log_posteriors = log_prior + log_M - np.log(np.exp(-pre_sigmoid_preds).sum(axis=1))\n",
    "    \n",
    "    return _fill_neg_inf_with_min(np.array(log_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example computation of posterior\n",
    "M = 200\n",
    "log_posterior = compute_log_posterior(THETAS, x, M, pre_sigmoid_model, scaler)\n",
    "\n",
    "bfp = THETAS[np.argmax(log_posterior)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,6), dpi=100)\n",
    "contour = ax.tricontourf(THETAS[:, 0], THETAS[:, 1], _fill_neg_inf_with_min(log_posterior), levels=20)\n",
    "cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r\"$ \\log_{10} p(\\theta | x) $\")  # Set colorbar label\n",
    "ax.scatter(np.log10(sin2_2theta), np.log10(delta_m2), color='white', label='Truth', marker='*', s=80)\n",
    "ax.scatter(*bfp, color='gray', marker='s', s=60, label=r'Maximum $\\it{a\\,\\,posteriori}$')\n",
    "ax.set_xlabel(r'$\\log_{10} \\sin^2 2 \\theta$')\n",
    "ax.set_ylabel(r'$\\log_{10} \\Delta m^2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example computation of posterior\n",
    "M = 200\n",
    "log_posterior_2 = compute_log_posterior(THETAS, x_2, M, pre_sigmoid_model, scaler)\n",
    "\n",
    "bfp_2 = THETAS[np.argmax(log_posterior_2)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,6), dpi=100)\n",
    "contour = ax.tricontourf(THETAS[:, 0], THETAS[:, 1], _fill_neg_inf_with_min(log_posterior_2), levels=20)\n",
    "cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r\"$ \\log_{10} p(\\theta | x) $\")  # Set colorbar label\n",
    "ax.scatter(np.log10(sin2_2theta_2), np.log10(delta_m2_2), color='white', label='Truth', marker='*', s=80)\n",
    "ax.scatter(*bfp_2, color='gray', marker='s', s=60, label=r'Maximum $\\it{a\\,\\,posteriori}$')\n",
    "ax.set_xlabel(r'$\\log_{10} \\sin^2 2 \\theta$')\n",
    "ax.set_ylabel(r'$\\log_{10} \\Delta m^2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dllh_estimate(theta, x, pre_sigmoid_model, scaler, M, posterior_gridsize):\n",
    "    \"\"\"\n",
    "    Estimate of dLLH between gridpoint theta and best fit point (MAP) for experimental realization x\n",
    "    \n",
    "    theta : array-like\n",
    "        Grid-point at which to compute test statistic\n",
    "    x : array-like\n",
    "        Raw experimental realization\n",
    "    pre_sigmoid_model\n",
    "        Trained subnetwork consisting of all layers except final activation\n",
    "    scaler\n",
    "        MinMaxScaler used for input data preparation\n",
    "    M : int\n",
    "        Number of samples to use in MC mean\n",
    "    posterior_gridsize : int\n",
    "        Gridsize to use for posterior computation (and thus granularity of MAP bfp estimate)\n",
    "    \"\"\"\n",
    "    # getting the MAP\n",
    "    log_sin2_2theta_grid = np.linspace(*LOG_SIN2_2THETA_BOUNDS, posterior_gridsize)\n",
    "    log_delta_m2_grid = np.linspace(*LOG_DELTA_M2_BOUNDS, posterior_gridsize)\n",
    "    thetas_for_posterior = np.array(\n",
    "        [[logsinsq, logdmsq] for logsinsq in log_sin2_2theta_grid for logdmsq in log_delta_m2_grid]\n",
    "    )\n",
    "    log_posterior = compute_log_posterior(thetas_for_posterior, x, M, pre_sigmoid_model, scaler)\n",
    "    bfp = thetas_for_posterior[np.argmax(log_posterior)]\n",
    "    \n",
    "    # computing likelihood ratio estimate\n",
    "    inp_scaled = _make_network_input(x.reshape(1, -1), theta.reshape(1, -1), bfp.reshape(1, -1), scaler)\n",
    "    return pre_sigmoid_model.predict(inp_scaled, verbose=0).flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab6df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the dLLH on the grid for an example\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "dllhs = [compute_dllh_estimate(th, x, pre_sigmoid_model, scaler, M, GRIDSIZE) for th in tqdm(THETAS)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,6), dpi=100)\n",
    "contour = ax.tricontourf(THETAS[:, 0], THETAS[:, 1], dllhs, levels=20)\n",
    "cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r\"$ \\Delta LLH $\")  # Set colorbar label\n",
    "ax.scatter(np.log10(sin2_2theta), np.log10(delta_m2), color='white', label='Truth', marker='*', s=80)\n",
    "ax.scatter(*bfp, color='gray', marker='s', s=60, label=r'Maximum $\\it{a\\,\\,posteriori}$')\n",
    "ax.set_xlabel(r'$\\log_{10} \\sin^2 2 \\theta$')\n",
    "ax.set_ylabel(r'$\\log_{10} \\Delta m^2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dbd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dllhs_2 = [compute_dllh_estimate(th, x_2, pre_sigmoid_model, scaler, M, GRIDSIZE) for th in tqdm(THETAS)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,6), dpi=100)\n",
    "contour = ax.tricontourf(THETAS[:, 0], THETAS[:, 1], dllhs_2, levels=20)\n",
    "cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r\"$ \\Delta LLH $\")  # Set colorbar label\n",
    "ax.scatter(np.log10(sin2_2theta_2), np.log10(delta_m2_2), color='white', label='Truth', marker='*', s=80)\n",
    "ax.scatter(*bfp_2, color='gray', marker='s', s=60, label=r'Maximum $\\it{a\\,\\,posteriori}$')\n",
    "ax.set_xlabel(r'$\\log_{10} \\sin^2 2 \\theta$')\n",
    "ax.set_ylabel(r'$\\log_{10} \\Delta m^2$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e916ba",
   "metadata": {},
   "source": [
    "## Building Confidence Level Estimates\n",
    "\n",
    "The procedure is pretty much the same as Feldman-Cousins; the only difference is that $\\Delta \\chi^2$ is not computed exactly; an estimate of $\\Delta LLH$ comes from the trained neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af055be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing critical dchi2 values. you only have to do this once!\n",
    "ALPHA = 0.9\n",
    "K = 100\n",
    "neg2dllh_estimate_crit_dict = {\n",
    "    \"sin2_2theta\": [],\n",
    "    \"delta_m2\": [],\n",
    "    \"-2dllh_estimate_crit\": [],\n",
    "}\n",
    "\n",
    "for th in tqdm(THETAS):\n",
    "    # simulate K experiments and compute dchi2s\n",
    "    dllh_estimates_temp = []\n",
    "    for _ in range(K):\n",
    "        x_temp, _ = simulate_counts(\n",
    "            sin2_2theta=10.**log10_sin2_2theta,\n",
    "            delta_m2=10.**log10_delta_m2\n",
    "        )\n",
    "        \n",
    "        dllh_estimate_temp = compute_dllh_estimate(\n",
    "            theta=th,\n",
    "            x=x_temp,\n",
    "            pre_sigmoid_model=pre_sigmoid_model,\n",
    "            scaler=scaler,\n",
    "            M=M,\n",
    "            posterior_gridsize=GRIDSIZE\n",
    "        )\n",
    "        dllh_estimates_temp.append(dllh_estimate_temp)\n",
    "        \n",
    "    # compute alpha percentile and append\n",
    "    dllh_estimate_crit = np.quantile(-2*dllh_estimate_temp, ALPHA) # this sign matters!\n",
    "    neg2dllh_estimate_crit_dict[\"sin2_2theta\"].append(10.**th[0])\n",
    "    neg2dllh_estimate_crit_dict[\"delta_m2\"].append(10.**th[1])\n",
    "    neg2dllh_estimate_crit_dict[\"-2dllh_estimate_crit\"].append(dllh_estimate_crit)\n",
    "    \n",
    "neg2dllh_estimate_crit_df = pd.DataFrame(neg2dllh_estimate_crit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d681e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "neg_2llh_estimates = np.array([\n",
    "    -2*compute_dllh_estimate(\n",
    "        theta=th,\n",
    "        x=x,\n",
    "        pre_sigmoid_model=pre_sigmoid_model,\n",
    "        scaler=scaler,\n",
    "        M=M,\n",
    "        posterior_gridsize=GRIDSIZE\n",
    "    )\n",
    "    for th in tqdm(THETAS)\n",
    "])\n",
    "\n",
    "neg_2llh_df = pd.DataFrame({\n",
    "    \"sin2_2theta\": 10.**THETAS[:, 0],\n",
    "    \"delta_m2\": 10.**THETAS[:, 1],\n",
    "    \"-2dllh_estimate\": neg_2llh_estimates,\n",
    "})\n",
    "\n",
    "acceptance_sbi_df = pd.merge(neg_2llh_df, neg2dllh_estimate_crit_df, on=['sin2_2theta', 'delta_m2'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97666d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,6), dpi=100)\n",
    "\n",
    "# SBI\n",
    "ax.tricontour(\n",
    "    acceptance_sbi_df[\"sin2_2theta\"],\n",
    "    acceptance_sbi_df[\"delta_m2\"],\n",
    "    acceptance_sbi_df[\"-2dllh_estimate\"] - acceptance_sbi_df[\"-2dllh_estimate_crit\"],\n",
    "    levels=[0.],\n",
    "    colors='r'\n",
    ")\n",
    "\n",
    "# Feldman-Cousins\n",
    "ax.tricontour(\n",
    "    acceptance_fc_df[\"sin2_2theta\"],\n",
    "    acceptance_fc_df[\"delta_m2\"],\n",
    "    acceptance_fc_df[\"delta_chi2\"] - acceptance_fc_df[\"delta_chi2_crit\"],\n",
    "    levels=[0.],\n",
    "    colors='purple',\n",
    ")\n",
    "\n",
    "\n",
    "contour = ax.tricontourf(\n",
    "    acceptance_sbi_df[\"sin2_2theta\"],\n",
    "    acceptance_sbi_df[\"delta_m2\"],\n",
    "    acceptance_sbi_df[\"-2dllh_estimate\"],\n",
    "    cmap='Greys',\n",
    "    levels=20\n",
    ")\n",
    "cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r\"$-2 \\Delta \\tilde{LLH} $\")  # Set colorbar label\n",
    "\n",
    "sc1 = ax.scatter(sin2_2theta, delta_m2, marker='*', label='Truth')\n",
    "\n",
    "bfp = tuple(acceptance_sbi_df[acceptance_sbi_df[\"-2dllh_estimate\"]==acceptance_sbi_df[\"-2dllh_estimate\"].min()][[\"sin2_2theta\", \"delta_m2\"]].values.flatten())\n",
    "\n",
    "sc2 = ax.scatter(*bfp, marker='o', facecolors='none', edgecolors='skyblue')\n",
    "\n",
    "bfp_fc = tuple(acceptance_fc_df[acceptance_fc_df[\"delta_chi2\"]==acceptance_fc_df[\"delta_chi2\"].min()][[\"sin2_2theta\", \"delta_m2\"]].values.flatten())\n",
    "\n",
    "sc3 = ax.scatter(*bfp_fc, marker='s', facecolors='none', edgecolors='skyblue')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r\"$\\sin^2 2 \\theta$\")\n",
    "ax.set_ylabel(r\"$\\Delta m^2$ [eV]$^2$\")\n",
    "\n",
    "contour_proxy = Line2D([0], [0], color='red', linestyle='-')\n",
    "contour_proxy_fc = Line2D([0], [0], color='purple', linestyle='-')\n",
    "\n",
    "# Combine legend handles and labels\n",
    "handles = [sc1, sc2, sc3, contour_proxy, contour_proxy_fc]\n",
    "labels = [\"Truth\", \"Best fit point (SBI)\", \"Best fit point (Feldman-Cousins)\", f\"{100*ALPHA}% CL, SBI\", f\"{100*ALPHA}% CL, Feldman-Cousins\"]\n",
    "\n",
    "ax.legend(handles, labels, framealpha=1.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example, an exclusion\n",
    "neg_2llh_estimates = np.array([\n",
    "    -2*compute_dllh_estimate(\n",
    "        theta=th,\n",
    "        x=x_2,\n",
    "        pre_sigmoid_model=pre_sigmoid_model,\n",
    "        scaler=scaler,\n",
    "        M=M,\n",
    "        posterior_gridsize=GRIDSIZE\n",
    "    )\n",
    "    for th in tqdm(THETAS)\n",
    "])\n",
    "\n",
    "neg_2llh_df = pd.DataFrame({\n",
    "    \"sin2_2theta\": 10.**THETAS[:, 0],\n",
    "    \"delta_m2\": 10.**THETAS[:, 1],\n",
    "    \"-2dllh_estimate\": neg_2llh_estimates,\n",
    "})\n",
    "\n",
    "acceptance_sbi_2_df = pd.merge(neg_2llh_df, neg2dllh_estimate_crit_df, on=['sin2_2theta', 'delta_m2'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,6), dpi=100)\n",
    "\n",
    "# SBI\n",
    "ax.tricontour(\n",
    "    acceptance_sbi_2_df[\"sin2_2theta\"],\n",
    "    acceptance_sbi_2_df[\"delta_m2\"],\n",
    "    acceptance_sbi_2_df[\"-2dllh_estimate\"] - acceptance_sbi_2_df[\"-2dllh_estimate_crit\"],\n",
    "    levels=[0.],\n",
    "    colors='r'\n",
    ")\n",
    "\n",
    "# Feldman-Cousins\n",
    "ax.tricontour(\n",
    "    acceptance_fc_2_df[\"sin2_2theta\"],\n",
    "    acceptance_fc_2_df[\"delta_m2\"],\n",
    "    acceptance_fc_2_df[\"delta_chi2\"] - acceptance_fc_2_df[\"delta_chi2_crit\"],\n",
    "    levels=[0.],\n",
    "    colors='purple',\n",
    ")\n",
    "\n",
    "\n",
    "contour = ax.tricontourf(\n",
    "    acceptance_sbi_2_df[\"sin2_2theta\"],\n",
    "    acceptance_sbi_2_df[\"delta_m2\"],\n",
    "    acceptance_sbi_2_df[\"-2dllh_estimate\"],\n",
    "    cmap='Greys',\n",
    "    levels=20\n",
    ")\n",
    "cbar = fig.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r\"$-2 \\Delta \\tilde{LLH} $\")  # Set colorbar label\n",
    "\n",
    "sc1 = ax.scatter(sin2_2theta_2, delta_m2_2, marker='*', label='Truth')\n",
    "\n",
    "bfp = tuple(acceptance_sbi_2_df[acceptance_sbi_2_df[\"-2dllh_estimate\"]==acceptance_sbi_2_df[\"-2dllh_estimate\"].min()][[\"sin2_2theta\", \"delta_m2\"]].values.flatten())\n",
    "\n",
    "sc2 = ax.scatter(*bfp, marker='o', facecolors='none', edgecolors='skyblue')\n",
    "\n",
    "bfp_fc = tuple(acceptance_fc_2_df[acceptance_fc_2_df[\"delta_chi2\"]==acceptance_fc_2_df[\"delta_chi2\"].min()][[\"sin2_2theta\", \"delta_m2\"]].values.flatten())\n",
    "\n",
    "sc3 = ax.scatter(*bfp_fc, marker='s', facecolors='none', edgecolors='skyblue')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r\"$\\sin^2 2 \\theta$\")\n",
    "ax.set_ylabel(r\"$\\Delta m^2$ [eV]$^2$\")\n",
    "\n",
    "contour_proxy = Line2D([0], [0], color='red', linestyle='-')\n",
    "contour_proxy_fc = Line2D([0], [0], color='purple', linestyle='-')\n",
    "\n",
    "# Combine legend handles and labels\n",
    "handles = [sc1, sc2, sc3, contour_proxy, contour_proxy_fc]\n",
    "labels = [\"Truth\", \"Best fit point (SBI)\", \"Best fit point (Feldman-Cousins)\", f\"{100*ALPHA}% CL, SBI\", f\"{100*ALPHA}% CL, Feldman-Cousins\"]\n",
    "\n",
    "ax.legend(handles, labels, framealpha=1.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9b714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
